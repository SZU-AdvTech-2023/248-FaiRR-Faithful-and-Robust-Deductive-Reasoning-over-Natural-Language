arch: roberta_large
train_batch_size: 16
eval_batch_size: 16
accumulate_grad_batches: 1
learning_rate: 1.0e-05
max_epochs: 10
optimizer: adamw
adam_epsilon: 1.0e-06
weight_decay: 0.01
lr_scheduler: linear_with_warmup
warmup_updates: 0.1
freeze_epochs: -1
gpus: 1
hf_name: roberta-large
